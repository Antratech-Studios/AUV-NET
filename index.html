
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 0px 0px 0px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 20px;
}
.affil-row {
    margin-top: 16px;
}
.conf-row {
    margin-top: 20px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 200px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>


<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis</title>
    <meta property="og:description" content="AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>AUV-Net: Learning Aligned UV Maps for Texture  Transfer and Synthesis</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
        	<div class="col-5 text-center">&nbsp;</div>
            <div class="col-5 text-center"><a href="https://czq142857.github.io/">Zhiqin Chen</a><sup>1,2</sup></div>
            <div class="col-5 text-center"><a href="https://kangxue.org/">Kangxue Yin</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,3,4</sup></div>
        	<div class="col-5 text-center">&nbsp;</div>
        </div>

        <div class="affil-row">
            <div class="col-4 text-center"><sup>1</sup>NVIDIA</div>
            <div class="col-4 text-center"><sup>2</sup>Simon Fraser university</div>
            <div class="col-4 text-center"><sup>3</sup>University of Toronto</div>
            <div class="col-4 text-center"><sup>4</sup>Vector Institute</div>
        </div>

        <div class="conf-row">
            <div class="venue text-center"><b>CVPR 2022</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/auvnet-paper.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="https://www.youtube.com/watch?v=UTzH8WB-Xl0">
                <span class="material-icons"> description </span> 
                 Video
            </a>
            <a class="supp-btn" href="https://github.com/nv-tlabs/AUV-NET">
                <span class="material-icons"> description </span> 
                  Code (coming soon)
            </a>
        </div></div>
    </div>
    </section>

    <section id="teaser">
        <figure style="width: 80%; float: center">
            <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                <source src="assets/Tsf-TGhead.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <p class="caption" style="text-align: center"> 
        	AUV-Net learns  <strong>aligned UV maps</strong> for a set of 3D shapes, enabling us to easily transfer textures to shapes by copying texture images. </p>
    </section>





    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>

        <p>
        In this paper, we address the problem of texture representation for 3D shapes for the challenging and underexplored tasks of texture transfer and synthesis. Previous works either apply spherical texture maps which may lead to large distortions, or use continuous texture fields that yield smooth outputs lacking details. We argue that the traditional way of representing textures with images and linking them to a 3D mesh via UV mapping is more desirable, since synthesizing 2D images is a well-studied problem. We propose AUV-Net which learns to embed 3D surfaces into a 2D aligned UV space, by mapping the corresponding semantic parts of different 3D shapes to the same location in the UV space. As a result, textures are aligned across objects, and can thus be easily synthesized by generative models of images. Texture alignment is learned in an unsupervised manner by a simple yet effective texture alignment module, taking inspiration from traditional works on linear subspace learning. The learned UV mapping and aligned texture representations enable a variety of applications including texture transfer, texture synthesis, and textured single view 3D reconstruction. We conduct experiments on multiple datasets to demonstrate the effectiveness of our method
		</p>
    </section>



    <section id="results">
        <h2>Results</h2>
        <hr>

    <section id="teaser-videos">
        <figure style="width: 80%; float: center">
            <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                <source src="assets/texture_transfer.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
            </p>
        </figure>
    </section>


        <hr>
        <figure style="width: 80%;">    
            <a href="assets/transfer_img_origin.jpg">
                <img width="100%" src="assets/transfer_img_origin.jpg">
            </a>       
        </figure> 
            <p class="caption"> <strong>Texture Transfer</strong>.  
            After training AUV-Net, we obtain aligned high-quality texture images for all training shapes. The fact that these texture images are aligned allows us to transfer
textures between two shapes by simply swapping
their texture images. In the above figure, we also show results on transferring textures to new shapes that are clearly different from the training shapes. In the bottom row of the Turbosquid Cars results, we show our method is able to correctly texture an over-simplified textureless car model. In the last row of RenderPeople results,  we show our method can transfer real scan textures to a cartoon shape.
            </p>

    </section>
    
    

    <section id="results">
        <hr>
    
    <section id="teaser-videos">
        <figure style="width: 80%; float: center">
            <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                <source src="assets/texture_synthesis.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
            </p>
        </figure>
    </section>
        <hr>

    
        <figure style="width: 100%;">
            <a href="assets/gen_img.jpg">
                <img width="100%" src="assets/gen_img.jpg">
            </a>
            <p class="caption"> <br /><strong>Texture Synthesis</strong>.
            A great advantage of having aligned
texture images is that it allows us to utilize existing 2D generative models to synthesize new textures for 3D shapes. We train StyleGAN2 in experiments and show results above. Note that, the holes on chairs are hallucinated via texture transparency (alpha channels in the texture images). 
            </p>
        </figure>
            <section id="results">

        <hr>
    
    <section id="teaser-videos">
        <figure style="width: 80%; float: center">
            <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                <source src="assets/svr.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
            </p>
        </figure>
    </section>


    
        <figure style="width: 50%;">
            <a href="assets/svr_img.jpg">
                <img width="100%" src="assets/svr_img.jpg">
            </a>
        </figure>

            <p class="caption"> <strong>Textured Single View Reconstruction</strong>.  
            With the aligned representation, we can condition texture synthesis on an input single-view image for 3D reconstruction. In this experiment, we use a 2D ResNet image encoder to predict the texture latent code and the shape code from the input image, a CNN decoder to predict the aligned
texture images from the texture latent code, and an IM-Net decoder to predict the geometry of the shape conditioned on the shape code.  
        <hr>

    </section>
    
    

    <section id="results">
        <h2>Method</h2>
        <hr>

        <figure style="width: 100%;">
            <a href="assets/network_img.jpg">
                <img width="100%" src="assets/network_img.jpg">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
<strong>Network architecture</strong> of our AUV-Net is shown above.
The core of AUV-Net is an unsupervised texture alignment module, inspired by Principal Component Analysis (PCA). Specifically, the basis generator networks predict basis images shared by all shapes in the training set. Then, those basis images are linearly combined with respect to the predicted per-shape coefficients to produce the texture images of each shape. Such a process forces the network to learn aligned texture images so that they can be effectively decomposed into basis images. 
The encoder predicts the shape code and the coefficients from the voxelized input point cloud. The UV mapper and the masker take as input the shape code and the query points from the input point cloud, and output the UV coordinates and the segmentation mask, respectively. The UV coordinates are fed into the two basis generators to obtain the basis colors for each query point, and the basis colors are multiplied by the predicted coefficients to generate the actual colors for each query point. Those colors from the two basis generators are selected by the predicted segmentation mask to produce the final colors.
            </p>
        </figure>
        




    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{chen2022AUVNET,
        title = {AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis}, 
        author = {Zhiqin Chen and Kangxue Yin and Sanja Fidler},
        booktitle = {The Conference on Computer Vision and Pattern Recognition (CVPR)},
        year = {2022}
}
</code></pre>
    </section>

<br />
    
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/auvnet-paper.pdf"><img class="screenshot" src="assets/paper_preview.png"></a>
            </div>
            <div style="width: 50%">
                <p><b>AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis</b></p>
                <p>Zhiqin Chen, Kangxue Yin,  Sanja Fidler</p>

                <div><span class="material-icons"> description </span><a href="assets/auvnet-paper.pdf"> Paper camera-ready</a></div>
                <div><span class="material-icons"> description </span><a href=""> arXiv version</a></div>
            </div>
        </div>
    </section>

</div>
</body>
</html>

